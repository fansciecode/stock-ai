# ğŸš€ AI Trading System - Microservices Architecture

## ğŸ—ï¸ ARCHITECTURE OVERVIEW

Our AI Trading System is now built with a clean microservices architecture for production scalability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLIENT        â”‚    â”‚     SERVER      â”‚    â”‚   AI MODEL      â”‚
â”‚   (Frontend)    â”‚â”€â”€â”€â–¶â”‚   (Backend)     â”‚â”€â”€â”€â–¶â”‚  (ML Engine)    â”‚
â”‚   Port: 8000    â”‚    â”‚   Port: 8001    â”‚    â”‚   Port: 8002    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Service Separation Benefits

- **ğŸŒ Client Service**: Handles UI, dashboard, and user interactions
- **ğŸ”§ Server Service**: Manages business logic, trading operations, and data
- **ğŸ¤– AI Model Service**: Dedicated ML inference with optimized resources
- **ğŸ“Š Independent Scaling**: Each service can scale based on demand
- **ğŸ›¡ï¸ Fault Isolation**: Issues in one service don't affect others

---

## ğŸš€ QUICK START

### Prerequisites
```bash
# Install Docker and Docker Compose
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Verify installation
docker --version
docker-compose --version
```

### 1. Local Development
```bash
# Clone the repository
git clone https://github.com/fansciecode/stock-ai.git
cd stock-ai

# Copy environment file
cp env.example .env

# Deploy microservices locally
./deployment/scripts/deploy-microservices.sh local
```

### 2. Production Deployment
```bash
# Deploy to cloud (AWS)
./deployment/scripts/deploy-microservices.sh cloud
```

---

## ğŸ“Š SERVICE DETAILS

### ğŸŒ Client Service (Port 8000)
**Purpose**: Frontend web application
- **Technology**: FastAPI + HTML/CSS/JS
- **Responsibilities**:
  - User interface and dashboard
  - Authentication and session management
  - Real-time data visualization
  - Trading controls and monitoring

**Endpoints**:
- `GET /` - Main dashboard
- `GET /health` - Health check
- `GET /api/signals` - Trading signals display
- `POST /api/trading/start` - Start trading
- `POST /api/trading/stop` - Stop trading

### ğŸ”§ Server Service (Port 8001)
**Purpose**: Backend API and business logic
- **Technology**: FastAPI + SQLAlchemy + Redis
- **Responsibilities**:
  - Trading session management
  - Order execution and routing
  - Data collection and storage
  - Business logic and rules
  - Exchange connectivity

**Endpoints**:
- `GET /api/system/status` - System status
- `GET /api/instruments` - Available instruments
- `POST /api/trading/start` - Create trading session
- `GET /api/portfolio/{user_id}` - User portfolio

### ğŸ¤– AI Model Service (Port 8002)
**Purpose**: Machine learning inference
- **Technology**: FastAPI + scikit-learn + joblib
- **Responsibilities**:
  - AI model loading and inference
  - Feature engineering
  - Prediction generation
  - Model performance monitoring
  - Caching and optimization

**Endpoints**:
- `POST /predict` - Generate predictions
- `GET /model/info` - Model information
- `GET /metrics` - Performance metrics
- `POST /model/retrain` - Retrain model

---

## ğŸ› ï¸ CONFIGURATION

### Environment Variables
```bash
# Service Configuration
ENVIRONMENT=production
CLIENT_PORT=8000
SERVER_PORT=8001
AI_MODEL_PORT=8002

# Database
DATABASE_URL=postgresql://user:pass@host:5432/db
REDIS_URL=redis://host:6379

# Trading APIs
BINANCE_API_KEY=your_api_key
BINANCE_SECRET_KEY=your_secret_key
```

### Docker Compose Configuration
```yaml
# deployment/docker/docker-compose.microservices.yml
version: '3.8'
services:
  client:
    build: Dockerfile.client
    ports: ["8000:8000"]
  
  server:
    build: Dockerfile.server
    ports: ["8001:8001"]
    
  ai-model:
    build: Dockerfile.ai-model
    ports: ["8002:8002"]
```

---

## ğŸ”§ DEVELOPMENT

### Running Individual Services
```bash
# Start AI Model service only
cd services/ai-model
python ai_model.py

# Start Server service only
cd services/server
python server.py

# Start Client service only
cd services/client
python client.py
```

### Service Communication
Services communicate via HTTP APIs:
```python
# Client -> Server
async with aiohttp.ClientSession() as session:
    response = await session.get(f"{SERVER_URL}/api/status")

# Server -> AI Model
async with aiohttp.ClientSession() as session:
    response = await session.post(f"{AI_MODEL_URL}/predict", json=data)
```

---

## ğŸ“Š MONITORING & HEALTH CHECKS

### Health Check Endpoints
```bash
# Check all services
curl http://localhost:8000/health  # Client
curl http://localhost:8001/health  # Server
curl http://localhost:8002/health  # AI Model
```

### Service Logs
```bash
# View all service logs
docker-compose -f deployment/docker/docker-compose.microservices.yml logs -f

# View specific service logs
docker-compose -f deployment/docker/docker-compose.microservices.yml logs client
docker-compose -f deployment/docker/docker-compose.microservices.yml logs server
docker-compose -f deployment/docker/docker-compose.microservices.yml logs ai-model
```

### Metrics
Each service exposes metrics:
- **Client**: User interactions, page views
- **Server**: Trading operations, API calls
- **AI Model**: Predictions made, inference time

---

## ğŸš€ DEPLOYMENT OPTIONS

### 1. Local Development
```bash
./deployment/scripts/deploy-microservices.sh local
```

### 2. Docker Swarm
```bash
docker swarm init
docker stack deploy -c deployment/docker/docker-compose.microservices.yml ai-trading
```

### 3. Kubernetes
```bash
kubectl apply -f deployment/k8s/
```

### 4. AWS ECS
```bash
# Configure AWS CLI first
aws configure

# Deploy to ECS
./deployment/scripts/deploy-microservices.sh cloud
```

---

## ğŸ”’ SECURITY

### Service Isolation
- Each service runs in its own container
- Network policies restrict inter-service communication
- Secrets management via environment variables

### API Security
- JWT token authentication
- Rate limiting on all endpoints
- Input validation and sanitization

---

## ğŸ“ˆ SCALING

### Horizontal Scaling
```bash
# Scale individual services
docker-compose -f deployment/docker/docker-compose.microservices.yml up --scale ai-model=3
docker-compose -f deployment/docker/docker-compose.microservices.yml up --scale server=2
```

### Resource Allocation
- **AI Model**: CPU/Memory intensive (2+ cores, 4+ GB RAM)
- **Server**: Moderate resources (1+ core, 2+ GB RAM)
- **Client**: Lightweight (1 core, 1 GB RAM)

---

## ğŸ¯ PRODUCTION FEATURES

### âœ… Current Features
- ğŸ—ï¸ Microservices architecture
- ğŸ³ Docker containerization
- ğŸ“Š 10,258+ instruments support
- ğŸ¤– Dedicated AI inference service
- ğŸ”„ Auto-scaling capabilities
- ğŸ›¡ï¸ Health monitoring
- ğŸ“ˆ Performance metrics

### ğŸš§ Future Enhancements
- ğŸ”„ Service mesh (Istio)
- ğŸ“Š Distributed tracing
- ğŸ›¡ï¸ Advanced security policies
- ğŸ“ˆ Auto-scaling based on ML load
- ğŸŒ Multi-region deployment

---

## ğŸŠ SUCCESS METRICS

**âœ… ACHIEVED:**
- ğŸ—ï¸ **Clean Architecture**: Separated client, server, and AI model
- ğŸš€ **Independent Scaling**: Each service scales independently
- ğŸ›¡ï¸ **Fault Isolation**: Issues contained to specific services
- ğŸ¤– **Optimized AI**: Dedicated ML service with performance tuning
- ğŸ“Š **Production Ready**: Docker, health checks, monitoring
- ğŸŒ **Cloud Native**: AWS/Kubernetes deployment ready

**ğŸ¯ READY FOR:**
- âœ… High-traffic production deployment
- âœ… Independent service scaling
- âœ… Technology-specific optimizations
- âœ… Advanced monitoring and observability
- âœ… Multi-cloud deployment
- âœ… Enterprise-grade operations
